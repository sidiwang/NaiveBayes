---
title: "Tutorial"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Introduction

The **NaiveBayes** package provides an efficient implementation of the popular Naive Bayes classifier. This package should be efficient, user friendly and written in <span style = "color:purple">base.R and Rcpp</span>. Like many other classifier packages, the *general* function **NaiveBayes** detects the class of each feature in the dataset and assumes possibly different distribution for each feature. *Predict* function uses a NaiveBayes model and a new data set to create the classifications. This can either be the **raw** probabilities generated by the NaiveBayes model or the **classes** themselves.

### What is Naive Bayes?

<span style = "color:purple"> Naive Bayes</span> is one of the most popular and simple [Machine Learning](https://en.wikipedia.org/wiki/Machine_learning "machine learning on Wikipedia") classification algorithms, the Naive Bayes Algorithm. It works on Bayes theorem of probability to predict the class of unknown data sets with an assumption of <span style = "color:purple">independece </span> among predictors. In simple terms, a <span style = "color:purple">Naive Bayes </span>classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature (i.e. assumes your  <span style = "color:purple">X</span> are all independent.)

<span style = "color:purple">Naive Bayes</span> model is easy to build and particularly useful for very large data sets. Along with simplicity, <span style = "color:purple">Naive Bayes</span> is known to outperform even highly sophisticated classification methods.

### What is Bayes Theorem?

<span style = "color:purple">Bayes Theorem</span> provides a way of calculating posterior probability

$P(C_k|x)=P(x|C_k)P(C_k)/P(x)$

where:

$P(c|x)$ is the posterior probability of *class* (c, target) given predictor (x, attributes).

$P(c)$ is the prior probability of *class*

$P(x|c)$ is the likelihodd which is the probability of *predictor* given *class*.

$P(x)$ is the prior probability of *predictor*

### How <span style = "color:purple">Naive Bayes</span> algorithem works?

Using the chain rule for repeated applications of the definition of conditional probability:


$P(C_k,x_1,...,x_n) = P(x_1, ..., x_n,C_k) = P(x_1|x_2,...,x_n,C_k)P(x_2|x_3,...,x_n,C_k)...P(x_n|C_k)P(C_k)$


given that all feature in **x** are <span style = "color:purple">mutually independent</span>, conditional on the category $C_k$. Under this assumption, we have:


$P(C_k|x_1,...,x_n)=1/Z \times P(x_1|C_k)\times P(x_2|C_k)\times ...\times P(x_n|C_k)\times P(C_k)$ 


where the evidence $Z = \sum_k P(C_k)P(x|C_k)$ is a scaling factor dependent only on $x_1,...x_n$, that is, a constant if the values of the feature variables are known. 
(Wikipedia: [Naive Bayes Classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier))

### Pros and Cons of <span style = "color:purple">Naive Bayes</span>

*Pros*:

+ Easy and fast in predicting lcass of test data. Also performs well in multi class prediction

+ When independence holds, <span style = "color:purple">Naive Bayes</span> performs better compare to other models like logistic regression and less training data is needed

+ Performs well in case of categorical input variables compare to numerical variables. For numerical variable, <span style = "color:purple">normal distribution</span> is assumed.

*Cons*:

+ If categorical variable has a category (in test data), which was not observed in training data. then model will not be able to make a prediction. To solve this, we can use the smoothing technique.

+ Assumption of independent predictors. in real life, it is almost impossible that we get a set of predictors which are completely independent.


## Installation

This package can be downloaded from Github: [Repository: NaiveBayes](https://github.com/sidiwang/NaiveBayes), and compiled on your local machine. After successful installation, the package can be used with:

```{r}
library(NaiveBayes)
```

## Main Functions

The general function `NaiveBayes()` detects the class of each feature in the dataset and assumes <span style = "color:purple">normal distribution</span> for continous variables.
The prediction function `predict.NaiveBayes()` can be called like many other classification packages: `predict(model_name, newdata, ...)`

## Numerical Underflow:

To avoid a numerical unverflow:

i.e. when n >> 0 in $P(C_k|x_1,...,x_n)=1/Z \times P(x_1|C_k)\times P(x_2|C_k)\times ...\times P(x_n|C_k)\times P(C_k)$

these calculations are performed on the log scale:

$log(P(C_k|x_1,...,x_n)) \propto log(P(C_k))+\sum_1^n log(P(x_i|C_k))$

Lastly, the class with the highest log-posterior probability is chosen to be the prediction, 

which is equivalent to  `predict(..., type = "class`)

if instead, the conditional class probabilities $P(c_K|X=x)$ are of the main interest, 

which then is equivalent to  `predict(..., type = "prob")`, 

then the log-posterior probabilities are transformed back to the original space and then normalized.

To speed up the calculation for large datasets, this package further simplied the formula above to matrix multiplication calculations in R.

## Other Statistical Terms
* Wikipedia: [Prior probability](https://en.wikipedia.org/wiki/Prior_probability)
* Wikipedia: [Categorical Distribution](https://en.wikipedia.org/wiki/Categorical_distribution)
* Wikipedia: [Normal Distribution](https://en.wikipedia.org/wiki/Normal_distribution)
* Wikipedia: [Additive Smoothing](https://en.wikipedia.org/wiki/Additive_smoothing)

## General Usage

+ model fitting style 1: `NaiveBayes(formula, data,...)`

```{r}
### Simulate data
n <- 100
set.seed(1)
data <- data.frame(class = sample(c("classA", "classB"), n, TRUE),
                   bern = sample(LETTERS[1:2], n, TRUE),
                   cat  = sample(letters[1:3], n, TRUE),
                   logical = sample(c(TRUE,FALSE), n, TRUE),
                   norm = rnorm(n),
                   count = rpois(n, lambda = c(5,15)))

# fit model
nb <- NaiveBayes(class ~ ., data) 
# check output
nb

```


+ model fitting style 2: `NaiveBayes(x, y,...)`

```{r}
# prepare data:
data(iris)
x = iris[ ,-5]
y = iris[ ,5]

# fit model
nb2 <- NaiveBayes(x, y) 
# check output
nb2

```


## Prediction

```{r}
# prepare data:
set.seed(2)
iris_shuffle = iris[sample(nrow(iris)),]
training = iris_shuffle[1:130,]
x = training[ ,-5]
y = training[ ,5]

testing = iris_shuffle[131:150, -5]

# fit model
nb3 <- NaiveBayes(x, y) 
# preidict
prediction = predict(nb3, testing)
# check output
prediction

```

## Performace Comparison

```{r}
#data(tweet)


```
